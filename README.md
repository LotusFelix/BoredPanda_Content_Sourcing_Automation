# ğŸ¼ BoredPanda Content Sourcing Automation

> **Production-ready AI-powered viral story discovery system** that automates content sourcing across 5 social platforms with English-only filtering, hybrid virality scoring, and comprehensive writer guidance.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production Ready-success.svg)]()

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [What's New](#-whats-new-latest-updates)
- [Architecture](#ï¸-architecture)
- [Quick Start](#-quick-start)
- [Technical Stack](#-technical-stack)
- [API Documentation](#-api-documentation)
- [Deployment](#-deployment)
- [Cost Analysis](#-cost-analysis)
- [Troubleshooting](#-troubleshooting)
- [Project Structure](#-project-structure)

---

## ğŸ¯ Overview

This system automates viral content discovery for BoredPanda's editorial team through:

1. **Multi-Platform Crawling**: Scrapes 5 platforms with English-only filtering
2. **Hybrid AI Scoring**: Rule-based metrics + GPT-4o-mini enhancement (0-100 scale)
3. **Intelligent Ranking**: Prioritizes high-virality potential stories
4. **Writer Guidance**: Actionable briefs with story angles and research tips

**Key Metrics**:
- âš¡ Processing Time: ~45-60 seconds per run
- ğŸ’° Cost per Run: ~$0.05 (Apify + OpenAI)
- ğŸ“Š Output: Top 20 English stories with AI analysis
- ğŸŒ Platforms: TikTok, Instagram, Facebook, Twitter, RSS

---

## âœ¨ Features

### ğŸš€ Core Functionality

#### **Multi-Platform Scraping** (English-Only Content)
- ğŸ“± **TikTok**: Hashtag scraping with `lang="en"` filter
- ğŸ“· **Instagram**: Hashtag scraping with language preference
- ğŸ‘¥ **Facebook**: Keyword search with English filter
- ğŸ¦ **Twitter**: Advanced search with `tweetLanguage="en"`
- ğŸ“° **RSS Feeds**: Pre-selected English sources (Reddit, NYT, Vulture)

#### **Dual-Layer Language Filtering**
1. **Platform-Level**: Native API language parameters
2. **Post-Processing**: Heuristic detection filters non-English content
   - Detects CJK, Arabic, Cyrillic scripts
   - Checks for common English words
   - Filters if >20% non-Latin characters

#### **Hybrid Virality Scoring** (0-100 Scale)
**Rule-Based Metrics** (90 points total):
- Velocity (25 pts): Engagements/hour - measures momentum
- Volume (20 pts): Total engagement count - measures reach
- Engagement Rate (20 pts): Engagements/followers - measures resonance
- Diversity (15 pts): Cross-platform presence (planned)
- Novelty (10 pts): Content freshness vs historical data (planned)
- Authority (10 pts): Source credibility based on follower count

**LLM Enhancement** (+/-10 points):
- GPT-4o-mini analyzes top 30 posts
- Generates comprehensive 3-part editorial brief:
  1. **Virality Analysis**: Why it's trending (hooks, timing, relatability)
  2. **BoredPanda Fit**: Alignment with brand values
  3. **Writer's Roadmap**: 5 actionable tips (headlines, research, visuals)

#### **Category Intelligence**
Research-backed hashtag mappings for **10 categories**:
- Funny, Animals, Relationships, Art & Design
- Entertainment, Curiosities, Lifestyle, Society
- Entertainment News, Politics

Each category has platform-optimized hashtags/keywords.

### ğŸ¨ Modern User Interface

**Input Form** (`index.html`):
- âœ… Multi-select category checkboxes (10 options)
- âœ… Multi-select source checkboxes (5 platforms)
- âœ… Date range slider (1-30 days back)
- âœ… Real-time validation
- âœ… Loading states with status updates

**Results Dashboard** (`dashboard.html`):
- ğŸ“Š Top 20 ranked story cards
- ğŸ¨ Color-coded score badges:
  - ğŸŸ¢ High (80-100): Strong viral potential
  - ğŸŸ¡ Medium (50-79): Moderate potential
  - ğŸ”´ Low (<50): Consider alternative angles
- ğŸ“ˆ Engagement metrics (likes, shares, comments)
- ğŸ“ Expandable AI briefs with formatted guidance
- ğŸ”— Direct source links for verification
- ğŸ“± Responsive design (mobile/tablet/desktop)

---

## ğŸ†• What's New: Latest Updates

### **v2.0 - Production-Ready Release** (December 2025)

#### ğŸ”§ Critical Bug Fixes
1. **Dependency Compatibility** âœ…
   - Fixed `apify-client` version (2.3.0 â†’ 1.6.0)
   - Resolved `proxies` parameter error
   - Root cause: Breaking API changes in apify-client 2.x
   
2. **OpenAI Client Fix** âœ…
   - Replaced `AsyncOpenAI` with synchronous `OpenAI`
   - Removed async/await from scoring methods
   - Improved reliability and error handling

3. **Twitter URL Mapping** âœ…
   - Fixed incorrect field mapping (`url` â†’ `tweetUrl`)
   - Twitter posts now link correctly to source tweets

4. **Dashboard HTML Syntax** âœ…
   - Fixed malformed `<link>` tag preventing CSS load
   - Resolved "No stories found" display issue

#### ğŸŒ English Language Filtering
- **Platform Filters**: Added `lang="en"` to all scrapers
- **Safety Net**: Created `language_filter.py` utility
  - Heuristic English detection
  - Filters non-Latin scripts (>20% threshold)
 - Logs filtered content for monitoring
  
#### ğŸ“Š Enhanced Monitoring
- Detailed logging at each pipeline stage
- Language filtering statistics
- Performance metrics (processing time, API costs)

---

## ğŸ—ï¸ Architecture

### System Design

```mermaid
graph TB
    A[User Interface] -->|HTTP/REST| B[FastAPI Backend]
    B --> C[Job Cache TTL 60min]
    B --> D[Category Mapper]
    B --> E[Scraper Orchestrator]
    
    E -->|Async| F1[TikTok Scraper lang=en]
    E -->|Async| F2[Instagram Scraper lang=en]
    E -->|Async| F3[Facebook Scraper lang=en]
    E -->|Async| F4[Twitter Scraper lang=en]
    E -->|Async| F5[RSS Scraper]
    
    F1 & F2 & F3 & F4 & F5 --> G[Data Aggregator]
    G --> H[Language Filter Safety Net]
    H --> I[Normalize & Deduplicate]
    I --> J[Virality Scorer]
    
    J -->|Top 30| K[GPT-4o-mini Enhancement]
    J -->|Remaining| L[Rule-Based Only]
    
    K & L --> M[Rank by Score]
    M --> N[Top 20 to Dashboard]
```

### Data Flow

1. **Input** â†’ Categories + Sources + Date Range
2. **Orchestration** â†’ Parallel async scraping (5 platforms)
3. **Normalization** â†’ Unified schema across platforms
4. **Language Filter** â†’ English-only content
5. **Deduplication** â†’ Remove URL duplicates
6. **Scoring** â†’ Rule-based (all) + LLM (top 30)
7. **Ranking** â†’ Sort by virality score
8. **Output** â†’ Top 20 stories with briefs

**Processing Pipeline**:
```
45-60 seconds total
â”œâ”€ Scraping: 20-30s (parallel)
â”œâ”€ Normalization: 2-3s
â”œâ”€ Language Filtering: 1-2s
â”œâ”€ Scoring (Rule-based): 3-5s
â””â”€ LLM Enhancement: 20-30s (30 API calls)
```

---

## ğŸš€ Quick Start

### Prerequisites

âœ… **Python 3.9+** ([Download](https://www.python.org/downloads/))  
âœ… **Apify API Token** ([Get Free Trial](https://apify.com/settings/integrations))  
âœ… **OpenAI API Key** ([Get Key](https://platform.openai.com/api-keys))

### Installation

```bash
# 1. Clone repository
git clone https://github.com/yourusername/content_sourcing_automation.git
cd content_sourcing_automation

# 2. Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate

# 3. Install dependencies (exact versions for compatibility)
pip install -r requirements.txt

# 4. Configure environment
copy .env.example .env
# Edit .env with your API keys
```

### Configuration

Create/edit `.env`:

```env
# Required
APIFY_API_TOKEN=apify_api_xxxxxxxxxxxxxxxxxxxxxxx
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxx

# Optional
ENVIRONMENT=development
PORT=8000
```

### Run Locally

```bash
# Start server
python -m backend.main

# Server runs at: http://localhost:8000
# API docs: http://localhost:8000/docs
```

### Verify Installation

```bash
# Test health endpoint
curl http://localhost:8000/health

# Expected response:
# {"status":"healthy","service":"BoredPanda Content Sourcing"}
```

---

## ğŸ”§ Technical Stack

### Core Dependencies

```txt
# Backend
fastapi==0.104.1          # FastAPI framework
uvicorn[standard]==0.24.0 # ASGI server
python-dotenv==1.0.0      # Environment management

# APIs (VERSION CRITICAL)
apify-client==1.6.0       # âš ï¸ Must be 1.x (2.x has breaking changes)
openai==1.3.5             # OpenAI GPT-4o-mini
aiohttp==3.9.1            # Async HTTP

# Data
pydantic==2.5.0           # Data validation
python-multipart==0.0.6   # Form data parsing
```

> **âš ï¸ WARNING**: `apify-client` version is critical. Do NOT upgrade to 2.x - it has breaking API changes incompatible with our initialization pattern.

### Frontend Stack

- **HTML5**: Semantic markup
- **CSS3**: Modern dark theme with gradients
- **Vanilla JavaScript**: No framework dependencies
- **SessionStorage**: Client-side state management

---

## ğŸ“– API Documentation

### Interactive Docs

Once running, visit:
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

### Quick Reference

#### 1. Health Check
```http
GET /health
```
Returns: Service status + cache statistics

#### 2. Get Configuration
```http
GET /config
```
Returns: Available categories and sources

#### 3. Start Scraping
```http
POST /scrape
Content-Type: application/json

{
  "categories": ["Funny", "Animals"],
  "sources": ["TikTok", "Twitter"],
  "days_back": 7
}
```
Returns: `job_id` for polling

#### 4. Get Results
```http
GET /results/{job_id}
```
Poll every 2 seconds until `status: "completed"`

Returns:
```json
{
  "status": "completed",
  "total_found": 20,
  "top_stories": [
    {
      "platform": "Twitter",
      "url": "https://twitter.com/...",
      "text": "Post content...",
      "category": "Funny",
      "virality_score": 85.3,
      "ai_brief": "**Why It's Viral:**...",
      "likes": 12500,
      "shares": 850,
      "comments": 420,
      "author": "username",
      "timestamp": "2025-12-16T10:30:00Z"
    }
  ]
}
```

---

## ğŸš¢ Deployment

### Option 1: Render.com (Recommended - Free Tier)

1. **Push to GitHub**
```bash
git init
git add .
git commit -m "Initial commit"
git remote add origin your-repo-url
git push -u origin main
```

2. **Deploy on Render**
   - Go to [render.com](https://render.com)
   - New â†’ Web Service
   - Connect GitHub repo
   
3. **Configure Build**:
```yaml
Build Command: pip install -r requirements.txt
Start Command: uvicorn backend.main:app --host 0.0.0.0 --port $PORT
```

4. **Environment Variables**:
```
APIFY_API_TOKEN=your_token
OPENAI_API_KEY=your_key
ENVIRONMENT=production
```

**Live URL**: `https://your-app.onrender.com`

> **Note**: Free tier sleeps after 15min inactivity (~30s cold start)

---

### Option 2: Docker

Create `Dockerfile`:
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Run:
```bash
docker build -t boredpanda-scraper .
docker run -p 8000:8000 \
  -e APIFY_API_TOKEN=your_token \
  -e OPENAI_API_KEY=your_key \
  boredpanda-scraper
```

---

## ğŸ’° Cost Analysis

### Current Scale (Prototype)

**Per Run**:
- Apify: $0.02 (100 results across 5 platforms)
- OpenAI: $0.03 (30 GPT-4o-mini calls)
- **Total: ~$0.05/run**

**Monthly** (100 runs): **~$5**

### Apify Breakdown
```
TikTok:    20 results Ã— $0.0001 = $0.002
Instagram: 20 results Ã— $0.0001 = $0.002
Facebook:  20 results Ã— $0.0001 = $0.002
Twitter:   20 results Ã— $0.0001 = $0.002
RSS:       20 results Ã— $0.0001 = $0.002
--------------------------------
Total:                    $0.020
```

### OpenAI Breakdown
```
Top 30 posts Ã— GPT-4o-mini ($0.001/call) = $0.030
```

---

## ğŸ”§ Troubleshooting

### Critical Issues

#### 1. `Client.__init__() got an unexpected keyword argument 'proxies'`

**Solution**: Ensure `apify-client==1.6.0` (NOT 2.x)
```bash
pip uninstall apify-client
pip install apify-client==1.6.0
```

**Root  Cause**: apify-client 2.x has breaking API changes

---

#### 2. Dashboard Shows "No stories found"

**Causes**:
 a) HTML syntax error (fixed in v2.0)
b) SessionStorage cleared
c) Job still processing

**Debug**:
```javascript
// In browser console:
sessionStorage.getItem('results')
// Should show JSON with stories
```

**Solution**: Hard refresh (Ctrl+Shift+R) and retry scrape

---

#### 3. Twitter URLs Not Working

**Solution**: Ensure Twitter scraper uses `tweetUrl` field
```python
# backend/utils/aggregator.py line 121
"url": item.get("tweetUrl", item.get("url", ""))
```

---

#### 4. Non-English Content Appearing

**Check**: Language filter is active
```python
# backend/orchestration/scraper_orchestrator.py
from backend.utils.language_filter import filter_english_posts
```

**Logs**: Should see "Filtered out N non-English posts"

---

### Common Installation Issues

**ModuleNotFoundError: backend**
```bash
# Always run from project root:
cd content_sourcing_automation
python -m backend.main
```

**Missing API Keys**
```bash
# Verify .env exists and has:
cat .env
# Should show APIFY_API_TOKEN and OPENAI_API_KEY
```

**Port Already in Use**
```bash
# Change port in .env:
PORT=8001

# Or kill process:
# Windows: netstat -ano | findstr :8000
# Mac/Linux: lsof -ti:8000 | xargs kill
```

---

## ğŸ“ Project Structure

```
content_sourcing_automation/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI app + endpoints
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ scraper_orchestrator.py  # Parallel scraping coordinator
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tiktok_scraper.py        # TikTok API wrapper
â”‚   â”‚   â”œâ”€â”€ instagram_scraper.py     # Instagram API wrapper
â”‚   â”‚   â”œâ”€â”€ facebook_scraper.py      # Facebook API wrapper
â”‚   â”‚   â”œâ”€â”€ twitter_scraper.py       # Twitter API wrapper
â”‚   â”‚   â””â”€â”€ rss_scraper.py           # RSS feed parser
â”‚   â”œâ”€â”€ scoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ virality_scorer.py       # Hybrid scoring engine
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ category_mapper.py       # Category â†’ hashtag mappings
â”‚       â”œâ”€â”€ aggregator.py            # Data normalization + ranking
â”‚       â”œâ”€â”€ cache.py                 # In-memory job storage (TTL)
â”‚       â””â”€â”€ language_filter.py       # ğŸ†• English detection utility
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html                   # Input form UI
â”‚   â”œâ”€â”€ dashboard.html               # Results display UI
â”‚   â”œâ”€â”€ styles.css                   # Modern dark theme
â”‚   â”œâ”€â”€ app.js                       # Form logic + API calls
â”‚   â””â”€â”€ dashboard.js                 # Results rendering
â”œâ”€â”€ .env.example                     # Environment template
â”œâ”€â”€ .gitignore                       # Git exclusions
â”œâ”€â”€ requirements.txt                 # Python dependencies (EXACT VERSIONS)
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ render.yaml                      # Render.com deployment config
```

---

## ğŸ¯ KPIs & Success Metrics

### Editorial Efficiency (Primary)
- â±ï¸ **Time Saved**: ~15 min per story discovered
- ğŸ¯ **Target**: 30% of published stories from automation
- ğŸ“Š **Goal**: 10+ hours saved per week

### Content Quality (Secondary)
- ğŸ² **Virality Accuracy**: Score correlation with actual performance
- ğŸ¯ **Target**: 70%+ correlation (predicted vs published)
- ğŸ“ˆ **Measurement**: Track stories 7 days post-publish

### System Reliability (Operational)
- âœ… **Uptime**: 95%+ (measured monthly)
- âš¡ **Success Rate**: 95%+ completed jobs
- â±ï¸ **Processing Time**: <60s (P95)

---

## ğŸ“ˆ Future Enhancements

### Phase 2 (Planned)
- [ ] Historical tracking (PostgreSQL)
- [ ] Duplicate detection across time
- [ ] Trending topic analysis
- [ ] Multi-language support (Spanish, Portuguese)
- [ ] Bulk export (CSV/JSON)
- [ ] Email alerts for high-score stories

### Phase 3 (Vision)
- [ ] Real-time monitoring dashboard
- [ ] Slack/Discord integration
- [ ] Custom scoring weights per editor
- [ ] A/B testing framework
- [ ] Performance analytics dashboard

---

## ğŸ™ Acknowledgments

- **Apify**: Robust scraping infrastructure
- **OpenAI**: GPT-4o-mini editorial analysis
- **BoredPanda**: Inspiring this automation project
- **Community**: Testing and feedback

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ“§ Support & Contact

**Issues**: [GitHub Issues](https://github.com/yourusername/content_sourcing_automation/issues)  
**Email**: your.email@example.com  
**Docs**: [Full Documentation](./docs/)

---

<div align="center">

**Built with â¤ï¸ for BoredPanda's Editorial Team**

ğŸ¼ [BoredPanda.com](https://boredpanda.com) | ğŸš€ [Live Demo](#) | ğŸ“š [API Docs](http://localhost:8000/docs)

</div>
